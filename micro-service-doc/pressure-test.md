压力测试注意的点：
1. 最好使用线上的数据和线上的环境
2. 不能使用模拟的请求，而是要使用线上的流量
3. 不要从一台服务器发起流量，这样很容易达到性能瓶颈，从而导致 QPS 上不去，最终影响测试的结果


全链路压测
流量的隔离：由于压力测试是在正式环境进行，所以需要区分压力测试流量和正式流量，这样可以针对压力测试的流量做单独的处理
风险的控制：尽量避免压力测试对于正常访问用户的影响


生成压测数据
在系统高峰期时，将入口流量拷贝一份，在经过一些流量清洗的工作之后，将数据存储在 HBase、MongoDB 这些 NoSQL 存储组件，或者亚马逊 S3 这些云存储服务中

当需要压测的时候，就可以从这个工厂中获取数据，将数据切分多份后下发到多个压测节点上

可以使用多种方式来实现流量的拷贝
1. 直接拷贝负载均衡服务器的访问日志，数据就以文本的方式写入到流量数据工厂中。这样产生的数据在发起压测时，需要自己写解析的脚本来解析访问日志，会增加压测时候的成本
2. 通过开源的工具（GoReplay）来实现流量的拷贝，它可以劫持本机某一个端口的流量，将它们记录在文件中，传送到流量数据工厂中。在压测时，也可以使用这个工具进行加速的流量回放，这样就可以实现对正式环境的压力测试了

在下发压测流量时，需要保证下发流量的节点与用户更加接近，起码不能和服务部署节点在同一个机房中，这样可以尽量保证压测数据的真实性


数据隔离
对于读取数据的请求，针对某些不能压测的服务或者组件，做 Mock 或者特殊的处理。这些 Mock 服务最好部署在真实服务所在的机房，这样可以尽量模拟真实的服务部署结构，提高压测结果的真实性

对于写入数据的请求，把压测流量产生的数据，写入到影子库，也就是和线上数据存储，完全隔离的一份存储系统中。针对不同的存储类型，使用不同的影子库的搭建方式：
1. 如果数据存储在 MySQL 中，可以在同一个 MySQL 实例，不同的 Schema 中创建一套和线上相同的库表结构，并且把线上的数据也导入进来
2. 而如果数据是放在 Redis 中，对压测流量产生的数据，增加一个统一的前缀，存储在同一份存储中
3. 还有一些数据会存储在 Elasticsearch 中，可以放在另外一个单独的索引表中


压力测试实施
设立一个压力测试的目标，在压测时，按照一定的步长，逐渐地增加流量。在增加一次流量之后，让系统稳定运行一段时间，观察系统在性能上的表现。如果发现依赖的服务或者组件出现了瓶颈，可以先减少压测流量，比如，回退到上一次压测的 QPS，保证服务的稳定，再针对此服务或者组件进行扩容，然后再继续增加流量压测

为了能够减少压力测试过程中，人力投入成本，可以开发一个流量监控的组件，在这个组件中，预先设定一些性能阈值：
容器的 CPU 使用率的阈值设定为 60%~70%
系统的平均响应时间的上限设定为 1 秒
系统慢请求的比例设置为 1%

当系统性能达到这个阈值之后，流量监控组件可以及时发现，并且通知压测流量下发组件减少压测流量，并且发送报警给到开发和运维的同学，开发和运维同学就迅速地排查性能瓶颈，在解决问题或者扩容之后再继续执行压测
